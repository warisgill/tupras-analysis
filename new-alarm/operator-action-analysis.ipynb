{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alarms import *\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_short_alarms = 2 * 60  #seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Reading CSV files and Preprocesssing (filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# files = [\"haziran2019.csv\",\"march2019.csv\",\"mayis2019.csv\",\"nisan2019.csv\"]\n",
    "files = [\"mayis2019.csv\"]\n",
    "files = [\"formatted-pre-2-\"+f for f in files]\n",
    "files_operator = [\"MayOperation_v2.xls\"]\n",
    "path = \"./data/new/\"\n",
    "\n",
    "\n",
    "# Reading alarms data\n",
    "\n",
    "f = files[0]\n",
    "print(\"==================== File : {} =============\".format(f))\n",
    "input_fname = f\n",
    "df_csv_alarms = pd.read_csv(path + input_fname, low_memory=False ,parse_dates=[\"StartTime\", \"EndTime\"])\n",
    "df_csv_alarms[\"TimeDelta\"] = df_csv_alarms[\"EndTime\"] - df_csv_alarms[\"StartTime\"]\n",
    "df_csv_alarms[\"TimeDelta\"] = df_csv_alarms[\"TimeDelta\"].apply(lambda arg: timedelta.total_seconds(arg)) \n",
    "df_alarms_filtered = df_csv_alarms[df_csv_alarms[\"TimeDelta\"]>filter_short_alarms] \n",
    "\n",
    "# df_temp = df_csv_alarms.loc[df_csv_alarms[\"SourceName\"].isin(df_excel_operator[\"SourceName\"].unique())]\n",
    "# df_alarms_filtered.info()\n",
    "\n",
    "\n",
    "# Reading operator data\n",
    "f = files_operator[0]\n",
    "print(\"==================== File : {} =============\".format(f))\n",
    "cols = [\"MachineName\",\"SourceName\",\"EventTime\",\"Message\",\"Severity\",\"Mask\",\"NewState\",\"EventType\",\"EventCategory\",\"AckReq\",\"ActorID\",\"Area\",\"Attributes\"]\n",
    "df_excel_operator = pd.read_excel(path+\"/operator-action/\"+f,usecols=cols)\n",
    "# print(\"Column  Type\")\n",
    "for col in df_excel_operator.columns:\n",
    "    # print(col, type(df_excel_operator[col][0]))\n",
    "    if isinstance(df_excel_operator[col][0],str):\n",
    "        df_excel_operator[col] = df_excel_operator[col].apply(lambda s: \" \".join(s.split()))\n",
    "\n",
    "# print(type(df_excel_operator[\"EventTime\"][0]))\n",
    "\n",
    "def changeDate(d):\n",
    "    d = d.replace(\".000000000\",\"\")\n",
    "    d = d.replace(\"/\",\"-\")\n",
    "    return parse(d)\n",
    "df_excel_operator[\"EventTime\"] = df_excel_operator[\"EventTime\"].apply(changeDate)\n",
    "\n",
    "assert len(df_alarms_filtered[\"MachineName\"].unique())==1\n",
    "assert len(df_excel_operator[\"MachineName\"].unique()) == 1\n",
    "assert df_alarms_filtered[\"MachineName\"].unique()[0]  == df_excel_operator[\"MachineName\"].unique()[0]\n",
    " \n",
    "talrms = len(df_alarms_filtered[\"SourceName\"].unique())\n",
    "toperator = len(df_excel_operator[\"SourceName\"].unique())\n",
    "\n",
    "\n",
    "temp_commons = [s for s in df_csv_alarms[\"SourceName\"].unique()  if s in df_excel_operator[\"SourceName\"].unique()]\n",
    "commons = [s for s in df_alarms_filtered[\"SourceName\"].unique()  if s in df_excel_operator[\"SourceName\"].unique()]\n",
    "\n",
    "# df_common_operator = df_excel_operator.loc[df_excel_operator[\"SourceName\"].isin(commons)]\n",
    "# assert len(df_common_operator][\"SourceName\"].unique())\n",
    "\n",
    "print(\" >> # of unique SourceNames in Alarms {}, Unique SourceNames in Operator Actions {}\".format(talrms,toperator))\n",
    "print(\" >> # of Common SourceName bw filtered alarms and non-filtered alarms {} {} with operator action, respectively\".format(len(commons),len(temp_commons)))\n",
    "print(\" >> # of filtered Alarms: {}, # of Operator Actions: {}\".format(df_alarms_filtered.shape[0],df_excel_operator.shape[0]))\n",
    "print(\"Raw Alarms \",df_csv_alarms.shape[0])\n",
    "print(\" >> Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Analyze the Common SourceNames (Alarms SourceNames and Operator Action Source Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> from pyvis.network import Network\n",
    "# >>> import networkx as nx\n",
    "# >>> nx_graph = nx.cycle_graph(10)\n",
    "# >>> nx_graph.nodes[1]['title'] = 'Number 1'\n",
    "# >>> nx_graph.nodes[1]['group'] = 1\n",
    "# >>> nx_graph.nodes[3]['title'] = 'I belong to a different group!'\n",
    "# >>> nx_graph.nodes[3]['group'] = 10\n",
    "# >>> nx_graph.add_node(20, size=20, title='couple', group=2)\n",
    "# >>> nx_graph.add_node(21, size=15, title='couple', group=2)\n",
    "# >>> nx_graph.add_edge(20, 21, weight=5)\n",
    "# >>> nx_graph.add_node(25, size=25, label='lonely', title='lonely node', group=3)\n",
    "# >>> nt = Network(\"500px\", \"500px\")\n",
    "# populates the nodes and edges data structures\n",
    "# >>> nt.from_nx(nx_graph)\n",
    "# >>> nt.show(\"nx.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construting Graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def case2Graph(df_alarms,df_operator, common_sources):\n",
    "    g = nx.Graph() # Undirected graph\n",
    "    df_operator = df_operator.loc[df_operator[\"SourceName\"].isin(common_sources)]\n",
    "    df_alarms = df_alarms.loc[df_alarms[\"SourceName\"].isin(common_sources)]\n",
    "    print(\">> Alarms:{}, Operator Actions:{}\".format(df_alarms.shape[0], df_operator.shape[0]))\n",
    "    for s in common_sources:\n",
    "        g.add_node(s,size=10,count=0,color=\"Green\", group = 1)\n",
    "    \n",
    "    for s in common_sources:\n",
    "        g.add_node(\"Operator->\"+s,size=10,count=0,color=\"Orange\", group = 1)\n",
    "\n",
    "    for s in df_alarms[\"SourceName\"]:\n",
    "        if g.has_node(s) == True:\n",
    "            g.nodes[s][\"size\"] += 0.2\n",
    "            g.nodes[s][\"count\"] += 1\n",
    "\n",
    "    for s in df_operator[\"SourceName\"]:\n",
    "        if g.has_node(\"Operator->\"+s) == True:\n",
    "            g.nodes[\"Operator->\"+s][\"size\"] += 0.2\n",
    "            g.nodes[\"Operator->\"+s][\"count\"] += 1\n",
    "\n",
    "    for s in common_sources:\n",
    "        g.add_edge(s,\"Operator->\"+s)\n",
    "    \n",
    "    for s in list(g.nodes):\n",
    "        g.nodes[s][\"title\"] = \"{}:{}\".format(s,g.nodes[s][\"count\"]) \n",
    "    \n",
    "    print(\">> {}\".format(nx.classes.function.info(g)))\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "ug = case2Graph(df_alarms_filtered, df_excel_operator, commons) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orange Nodes are operator action nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = Network(\"500px\", \"100%\", notebook=True)\n",
    "nt.from_nx(ug)\n",
    "# nt.show_buttons()\n",
    "# nt.repulsion()\n",
    "nt.show(\"nt.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3: Relating alarms with actions\n",
    "\n",
    "### Process data to cross check this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def case3RelatingAlarmsWithActions(df_alarms,df_actions, common_sources, filter1 = (60*60) * 5):\n",
    "    print(\">> Alarms:{}, Operator Actions:{}\".format(df_alarms.shape[0], df_actions.shape[0]))    \n",
    "    \n",
    "    g = nx.DiGraph() # Directed graph\n",
    "    \n",
    "    #---------- Adding Nodes----------------\n",
    "    for s in df_alarms[\"SourceName\"]:\n",
    "        if g.has_node(s) == True:\n",
    "            # g.nodes[s][\"size\"] += 0.2\n",
    "            g.nodes[s][\"count\"] += 1\n",
    "        else:\n",
    "            g.add_node(s,size=10,count=1,color=\"Black\", group = 1) \n",
    "\n",
    "    for s in df_actions[\"SourceName\"]:\n",
    "        if g.has_node(\"Operator->\"+s) == True:\n",
    "            g.nodes[\"Operator->\"+s][\"size\"] += 0.2\n",
    "            g.nodes[\"Operator->\"+s][\"count\"] += 1\n",
    "        else:\n",
    "            g.add_node(\"Operator->\"+s,size=10,count=1,color=\"Orange\", group = 1)    \n",
    "\n",
    "    #------------------ ADDING EDGES --------------------------\n",
    "    alarms_by_etime = [a for a in sorted(df_alarms.to_dict(orient=\"records\"), key=lambda arg: arg[\"EndTime\"], reverse=False)]\n",
    "    actions_by_time = [r for r in sorted(df_actions.to_dict(orient=\"records\"), key=lambda arg: arg[\"EventTime\"], reverse=False)]\n",
    "    \n",
    "    for i in range(len(actions_by_time)):\n",
    "         action = actions_by_time[i]\n",
    "        #  if i%100 == 0:\n",
    "        #     print(i,end=\",\")\n",
    "         for j in range(len(alarms_by_etime)):            \n",
    "            alarm = alarms_by_etime[j]\n",
    "            if action[\"EventTime\"] > alarm[\"StartTime\"] and action[\"EventTime\"] <= alarm[\"EndTime\"] and timedelta.total_seconds(alarm[\"EndTime\"]-action[\"EventTime\"])<filter1:\n",
    "                if g.has_edge(\"Operator->\"+action[\"SourceName\"],alarm[\"SourceName\"])==False:  \n",
    "                    g.add_edge(\"Operator->\"+action[\"SourceName\"],alarm[\"SourceName\"],weight=1)\n",
    "                else:\n",
    "                    g.edges[\"Operator->\"+action[\"SourceName\"],alarm[\"SourceName\"]][\"weight\"] +=1\n",
    "    \n",
    "\n",
    "    for s in list(g.nodes):\n",
    "        g.nodes[s][\"title\"] = \"{}:{}\".format(s,g.nodes[s][\"count\"])\n",
    "\n",
    "    for e in list(g.edges):\n",
    "        g.edges[e][\"title\"] = \"{}:{}\".format(e,g.edges[e][\"weight\"])\n",
    "\n",
    "    remove_edges = []\n",
    "\n",
    "    for edge in list(g.edges):\n",
    "        if g.edges[edge][\"weight\"] <=100:\n",
    "            remove_edges.append(edge)\n",
    "\n",
    "    g.remove_edges_from(remove_edges)\n",
    "\n",
    "    print(\">> {}\".format(nx.classes.function.info(g)))\n",
    "    return g\n",
    "\n",
    "dG_case3 = case3RelatingAlarmsWithActions(df_alarms_filtered, df_excel_operator, commons) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orange nodes are the operator action nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = Network(\"500px\", \"100%\", notebook=True)\n",
    "nt.from_nx(dG_case3)\n",
    "nt.show_buttons()\n",
    "# nt.repulsion()\n",
    "nt.show(\"nt.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes_dict = {}\n",
    "for s in list(dG_case3.nodes):\n",
    "    nodes_dict[s] = {\"count\":dG_case3.nodes[s][\"count\"], \"outd\" : dG_case3.out_degree(s,\"weight\"), \"ind\":dG_case3.in_degree(s,\"weight\"), \"totald\": dG_case3.degree(s,\"weight\")}\n",
    "\n",
    "nodes_dict = {k:v for k, v in sorted(nodes_dict.items(), key=lambda arg: arg[1][\"ind\"], reverse=True) if v[\"count\"]>20 and v[\"totald\"]>20}\n",
    "print(nodes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 (Extra): Find the most important SourceName in the Graph using PageRank or Centrality Algos\n",
    "Not considering operator data overhere\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def case4FindImportantSensorNodes(df_alarms, common_sources, tfilter1 = 60):\n",
    "    print(\">> Alarms:{} \".format(df_alarms.shape[0]))\n",
    "    G = nx.DiGraph() # Directed Graph\n",
    "\n",
    "    # # Adding Nodes\n",
    "    for s in df_alarms[\"SourceName\"]:\n",
    "        if G.has_node(s) == True:\n",
    "            G.nodes[s][\"size\"] += 0.1\n",
    "            G.nodes[s][\"count\"] += 1\n",
    "        else:\n",
    "            G.add_node(s,size=5,count =1)\n",
    "    \n",
    "    for s in list(G.nodes):\n",
    "        G.nodes[s][\"title\"] = \"{}:{}\".format(s,G.nodes[s][\"count\"])\n",
    "\n",
    "    start_records = [v for v in sorted(df_alarms.to_dict(orient=\"records\"), key=lambda arg: arg[\"StartTime\"], reverse=False)]\n",
    "    for i in range(len(start_records)):\n",
    "        prevd = start_records[i]\n",
    "        for j in range(i+1,len(start_records)):        \n",
    "            nextd = start_records[j]\n",
    "            if timedelta.total_seconds(nextd[\"StartTime\"]-prevd[\"StartTime\"]) > tfilter1: # if next alarm is not triggered within tfilter1 duration then break \n",
    "                break \n",
    "            if nextd[\"SourceName\"] != prevd[\"SourceName\"] and nextd[\"StartTime\"] >= prevd[\"StartTime\"]: \n",
    "               if nextd[\"EndTime\"] <= prevd[\"EndTime\"] or timedelta.total_seconds(nextd[\"EndTime\"]-prevd[\"EndTime\"]) < tfilter1:\n",
    "                    if G.has_edge(prevd[\"SourceName\"],nextd[\"SourceName\"]) == False:\n",
    "                        G.add_edge(prevd[\"SourceName\"],nextd[\"SourceName\"],color=\"Red\",weight=1)\n",
    "                    else:\n",
    "                        G.edges[prevd[\"SourceName\"],nextd[\"SourceName\"]][\"weight\"] +=1\n",
    "\n",
    "\n",
    "    # remove_edges = []\n",
    "\n",
    "    for edge in list(G.edges):\n",
    "        G.edges[edge][\"title\"] = \"{}\".format(G.edges[edge][\"weight\"])\n",
    "        # G.edges[edge][\"weight\"] = 1\n",
    "        # if G.edges[edge][\"weight\"] <=2:\n",
    "        #     remove_edges.append(edge)\n",
    "\n",
    "    # G.remove_edges_from(remove_edges)\n",
    "    \n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    print(\">> {}\".format(nx.classes.function.info(G)))\n",
    "    return G \n",
    "\n",
    "dG_case4 = case4FindImportantSensorNodes(df_alarms_filtered, commons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importan SourceNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Google PageRank Algo\n",
    "result = nx.algorithms.link_analysis.pagerank_alg.pagerank(dG_case4,weight=\"weight\",max_iter=100000)\n",
    "result = {k:float(format(v, '.4f')) for k,v in sorted(result.items(), key=lambda arg: arg[1], reverse=True)}\n",
    "print(\">> Page Rank (Highest to Lowest) :\",list(result.items())[:50])\n",
    "\n",
    "print(\"              --------------------------------------------------\")\n",
    "\n",
    "# Eigenvector Centrality Algo\n",
    "result = nx.eigenvector_centrality(dG_case4, weight=\"weight\")\n",
    "result = {k:float(format(v, '.4f')) for k,v in sorted(result.items(), key=lambda arg: arg[1], reverse=True)}\n",
    "print(\">> Eigenvector Centrality (Highest to Lowest) :\",list(result.items())[:50])\n",
    "\n",
    "# result = nx.closeness_centrality(G)\n",
    "# result = {k:float(format(v, '.4f')) for k,v in sorted(result.items(), key=lambda arg: arg[1], reverse=True)}\n",
    "# print(\"Closeness Centrality:\",list(result.items())[:50])\n",
    "# # closeness_centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important nodes based on incoming and outgoing edges\n",
    "\n",
    "outgoing-> triggering other alarms;\n",
    "\n",
    "incoming -> triggered after another alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hits Algo\n",
    "h, a = nx.hits(dG_case4)\n",
    "result = h\n",
    "result = {k:float(format(v, '.4f')) for k,v in sorted(result.items(), key=lambda arg: arg[1], reverse=True)}\n",
    "print(\">> Hub => Outgoing Edges: Based on out_degree (max to min):\",list(result.items())[:50])\n",
    "\n",
    "print(\"              --------------------------------------------------\")\n",
    "\n",
    "result = a\n",
    "result = {k:float(format(v, '.4f')) for k,v in sorted(result.items(), key=lambda arg: arg[1], reverse=True)}\n",
    "print(\"Auth => Incoming Edges: Based on in_degree (max to min):\",list(result.items())[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes_dict = {}\n",
    "for s in list(dG_case4.nodes):\n",
    "    nodes_dict[s] = {\"count\":dG_case4.nodes[s][\"count\"], \"outd\" : dG_case4.out_degree(s,\"weight\"), \"ind\":dG_case4.in_degree(s,\"weight\"), \"totald\": dG_case4.degree(s,\"weight\")}\n",
    "\n",
    "nodes_dict = {k:v for k, v in sorted(nodes_dict.items(), key=lambda arg: arg[1][\"ind\"], reverse=True) if v[\"count\"]>4 and v[\"totald\"]>1}\n",
    "print(nodes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undirected = ipycytoscape.CytoscapeWidget()\n",
    "# undirected.graph.add_graph_from_networkx(G )\n",
    "# undirected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nt.enable_physics(False)\n",
    "# nt.set_options('{nodes: {shape: \"dot\",size: 30,font: {size: 32,color: \"#ffffff\"},borderWidth: 2},edges: {width: 2}}')\n",
    "# nt.toggle_physics(False)\n",
    "# nt.toggle_stabilization(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ranks_dict_alarms = df_common_alarms[\"SourceName\"].value_counts()\n",
    "# ranks_dict_actions =df_common_operator[\"SourceName\"].value_counts()\n",
    "\n",
    "# for sname in sorted(df_common_alarms[\"SourceName\"].unique()):\n",
    "#     if ranks_dict_actions[sname]== ranks_dict_alarms[sname]:\n",
    "#         print(\">{}, Operator = {}, Alarms = {}\".format(sname, ranks_dict_actions[sname], ranks_dict_alarms[sname]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getDF (df, col, values_arra):\n",
    "#     return df.loc[df[col].isin(values_arra)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sname = \"47FIC1569\"\n",
    "\n",
    "# getDF(df_common_alarms,\"SourceName\", [sname]).sort_values(by=[\"StartTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getDF(df_common_operator,\"SourceName\", [sname]).sort_values(by=[\"EventTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [\"MachineName\",\"SourceName\",\"EventTime\", \"Message\",\"MessageType\",\"Quality\",\"Condition\",\"Mask\",\"NewState\",\"Status\"]\n",
    "# df_normal = pd.read_csv(path+\"formatted-pre-1-mayis2019.csv\",parse_dates=[\"EventTime\"],usecols=cols)\n",
    "# getDF(df_normal,\"SourceName\",[sname]).sort_values(by=[\"EventTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tupras': conda)",
   "language": "python",
   "name": "python38364bittuprascondad3f64d142d7848d8b5ff812324ef08eb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
